import os
import json
import requests
import pandas as pd
from flask import Flask, request, render_template, jsonify
from twilio.twiml.messaging_response import MessagingResponse
from twilio.rest import Client
import google.generativeai as genai
from dotenv import load_dotenv
import redis
from datetime import datetime
import logging
from config_agente import obtener_system_prompt, obtener_configuracion, obtener_limites, obtener_mensajes, validar_consulta

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Fix: Usar gemini-1.5-flash (modelo disponible en API gratuita) - v3

# Inicializar Flask
app = Flask(__name__)

# Configurar APIs de IA
gemini_api_key = os.getenv('GEMINI_API_KEY')
siliconflow_api_key = os.getenv('SILICONFLOW_API_KEY', 'sk-qurqyhsbdzeduzrgfnukhtlzkwmdxinbtdvhaadudsrucecq')

# Configurar Gemini
if gemini_api_key:
    genai.configure(api_key=gemini_api_key)
    logger.info("Gemini API configurada correctamente")
else:
    logger.warning("GEMINI_API_KEY no configurada")

# Configurar SiliconFlow
logger.info("SiliconFlow API configurada correctamente")

# Variable global para el proveedor de IA activo
PROVEEDOR_IA_ACTIVO = "gemini"  # "gemini" o "siliconflow"

# Verificar qu√© modelos est√°n disponibles
def listar_modelos_disponibles():
    """Lista los modelos disponibles en la API de Gemini"""
    try:
        models = genai.list_models()
        available_models = []
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append(model.name)
        logger.info(f"Modelos disponibles: {available_models}")
        return available_models
    except Exception as e:
        logger.error(f"Error listando modelos: {e}")
        return []

# Verificar que el modelo est√© disponible
def verificar_modelo_disponible():
    """Verifica que el modelo de Gemini est√© disponible"""
    try:
        # Primero listar modelos disponibles
        modelos_disponibles = listar_modelos_disponibles()
        
        # Intentar con diferentes modelos en orden de preferencia
        modelos_a_probar = [
            'gemini-1.5-flash',
            'gemini-1.5-flash-001', 
            'gemini-1.5-flash-002',
            'gemini-1.5-pro',
            'gemini-pro'
        ]
        
        for modelo in modelos_a_probar:
            try:
                model = genai.GenerativeModel(modelo)
                response = model.generate_content("Hola")
                logger.info(f"Modelo {modelo} verificado correctamente")
                return True, modelo
            except Exception as e:
                logger.warning(f"Modelo {modelo} no disponible: {e}")
                continue
        
        logger.error("Ning√∫n modelo de Gemini est√° disponible")
        return False, None
        
    except Exception as e:
        logger.warning(f"Advertencia verificando modelo: {e}")
        logger.warning("La aplicaci√≥n continuar√°, pero el modelo podr√≠a no estar disponible")
        return False, None

# Cargar configuraci√≥n del agente
def cargar_configuracion_dinamica():
    """Carga configuraci√≥n din√°mica si existe, sino usa la por defecto"""
    try:
        if os.path.exists('config_dinamico.json'):
            with open('config_dinamico.json', 'r', encoding='utf-8') as f:
                config_dinamico = json.load(f)
            logger.info("Configuraci√≥n din√°mica cargada desde config_dinamico.json")
            return (
                config_dinamico.get("system_prompt", obtener_system_prompt()),
                config_dinamico.get("config_agente", obtener_configuracion()),
                config_dinamico.get("limites", obtener_limites()),
                config_dinamico.get("mensajes", obtener_mensajes())
            )
        else:
            logger.info("Usando configuraci√≥n por defecto")
            return (
                obtener_system_prompt(),
                obtener_configuracion(),
                obtener_limites(),
                obtener_mensajes()
            )
    except Exception as e:
        logger.error(f"Error cargando configuraci√≥n din√°mica: {e}")
        logger.info("Usando configuraci√≥n por defecto como fallback")
        return (
            obtener_system_prompt(),
            obtener_configuracion(),
            obtener_limites(),
            obtener_mensajes()
        )

SYSTEM_PROMPT, CONFIG, LIMITES, MENSAJES = cargar_configuracion_dinamica()

# Variable global para almacenar el modelo que funciona
MODELO_DISPONIBLE = None

def obtener_modelo_funcional():
    """Obtiene un modelo de Gemini que funcione"""
    global MODELO_DISPONIBLE
    
    if MODELO_DISPONIBLE:
        return MODELO_DISPONIBLE
    
    # Intentar con diferentes modelos en orden de preferencia
    # Basado en los modelos disponibles seg√∫n el debug
    modelos_a_probar = [
        'gemini-1.5-flash-latest',
        'gemini-1.5-flash',
        'gemini-1.5-pro-latest',
        'gemini-1.5-pro',
        'gemini-2.0-flash',
        'gemini-2.0-flash-001',
        'gemini-2.5-flash',
        'gemini-pro-latest',
        'gemini-pro'
    ]
    
    for modelo in modelos_a_probar:
        try:
            logger.info(f"Probando modelo: {modelo}")
            model = genai.GenerativeModel(modelo)
            logger.info(f"Modelo {modelo} inicializado correctamente")
            
            # Hacer una prueba r√°pida
            response = model.generate_content("test")
            logger.info(f"Modelo {modelo} respondi√≥ correctamente: {response.text[:50]}...")
            
            MODELO_DISPONIBLE = modelo
            logger.info(f"Modelo funcional encontrado: {modelo}")
            return modelo
        except Exception as e:
            logger.error(f"Modelo {modelo} fall√≥ con error: {str(e)}")
            logger.error(f"Tipo de error: {type(e).__name__}")
            continue
    
    logger.error("Ning√∫n modelo de Gemini est√° disponible")
    return None

# Configurar Twilio
twilio_client = Client(
    os.getenv('TWILIO_ACCOUNT_SID'),
    os.getenv('TWILIO_AUTH_TOKEN')
)

# Configurar Redis para memoria (opcional)
try:
    redis_client = redis.from_url(os.getenv('REDIS_URL', 'redis://localhost:6379'))
    redis_client.ping()
    logger.info("Redis conectado exitosamente")
except:
    redis_client = None
    logger.warning("Redis no disponible, usando memoria local")

# Cargar datos del Excel
def cargar_inventario():
    """Carga el archivo Excel de inventario"""
    try:
        df = pd.read_excel('inventario.xlsx')
        logger.info(f"Inventario cargado: {len(df)} productos")
        return df
    except FileNotFoundError:
        logger.error("Archivo inventario.xlsx no encontrado")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"Error cargando inventario: {e}")
        return pd.DataFrame()

# Funci√≥n para obtener/crear sesi√≥n de chat
def obtener_sesion_chat(usuario_id):
    """Obtiene o crea una sesi√≥n de chat para el usuario"""
    if redis_client:
        # Intentar obtener sesi√≥n desde Redis
        sesion_data = redis_client.get(f"chat_session_{usuario_id}")
        if sesion_data:
            return json.loads(sesion_data)
    
    # Crear nueva sesi√≥n
    modelo_funcional = obtener_modelo_funcional()
    if not modelo_funcional:
        logger.error("No hay modelos disponibles para crear sesi√≥n de chat")
        return None
    
    model = genai.GenerativeModel(modelo_funcional)
    chat = model.start_chat(history=[])
    
    # Guardar en Redis si est√° disponible
    if redis_client:
        redis_client.setex(f"chat_session_{usuario_id}", 3600, json.dumps({
            'history': chat.history
        }))
    
    return chat

def guardar_sesion_chat(usuario_id, chat):
    """Guarda la sesi√≥n de chat en Redis"""
    if redis_client:
        redis_client.setex(f"chat_session_{usuario_id}", 3600, json.dumps({
            'history': chat.history
        }))

def consultar_con_siliconflow(query_texto, df):
    """Consulta usando SiliconFlow API"""
    try:
        # Crear contexto para SiliconFlow
        contexto = f"""
        {SYSTEM_PROMPT}
        
        INVENTARIO (10 productos):
        {df.to_string()}
        
        CONSULTA: {query_texto}
        
        Responde solo con informaci√≥n del inventario.
        """
        
        # Configurar headers para SiliconFlow
        headers = {
            'Authorization': f'Bearer {siliconflow_api_key}',
            'Content-Type': 'application/json'
        }
        
        # Datos para la API
        data = {
            "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
            "messages": [
                {
                    "role": "user",
                    "content": contexto
                }
            ],
            "max_tokens": 500,
            "temperature": 0.7
        }
        
        # Hacer la petici√≥n
        response = requests.post(
            'https://api.siliconflow.cn/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            respuesta = result['choices'][0]['message']['content']
            
            # Limitar longitud de respuesta
            if len(respuesta) > CONFIG["max_respuesta_caracteres"]:
                respuesta = respuesta[:CONFIG["max_respuesta_caracteres"]] + "..."
            
            return respuesta
        else:
            logger.error(f"Error en SiliconFlow API: {response.status_code} - {response.text}")
            return MENSAJES["error_general"]
            
    except Exception as e:
        logger.error(f"Error consultando con SiliconFlow: {e}")
        return MENSAJES["error_general"]

def consultar_excel(query_texto, df):
    """Consulta el Excel usando el proveedor de IA configurado"""
    if df.empty:
        return MENSAJES["error_general"]
    
    # Validar consulta de seguridad
    es_valida, mensaje_error = validar_consulta(query_texto)
    if not es_valida:
        return mensaje_error
    
    # Usar el proveedor configurado
    if PROVEEDOR_IA_ACTIVO == "siliconflow":
        logger.info("Usando SiliconFlow para consulta")
        return consultar_con_siliconflow(query_texto, df)
    else:
        logger.info("Usando Gemini para consulta")
        return consultar_con_gemini(query_texto, df)

def consultar_con_gemini(query_texto, df):
    """Consulta el Excel usando Gemini para interpretar la consulta"""
    # Crear contexto para Gemini con system prompt
    contexto_excel = f"""
    {SYSTEM_PROMPT}
    
    DATOS DEL INVENTARIO:
    Columnas disponibles: {list(df.columns)}
    
    Inventario completo:
    {df.to_string()}
    
    CONSULTA DEL USUARIO: {query_texto}
    
    Responde siguiendo las reglas establecidas y usando solo la informaci√≥n del inventario.
    """
    
    try:
        modelo_funcional = obtener_modelo_funcional()
        if not modelo_funcional:
            logger.error("No hay modelos disponibles para consultar Excel")
            return MENSAJES["error_general"]
        
        model = genai.GenerativeModel(modelo_funcional)
        response = model.generate_content(contexto_excel)
        
        # Limitar longitud de respuesta
        respuesta = response.text
        if len(respuesta) > CONFIG["max_respuesta_caracteres"]:
            respuesta = respuesta[:CONFIG["max_respuesta_caracteres"]] + "..."
        
        return respuesta
    except Exception as e:
        logger.error(f"Error consultando Excel con Gemini: {e}")
        return MENSAJES["error_general"]

def procesar_archivo_multimodal(url_archivo, tipo_archivo, usuario_id, df):
    """Procesa archivos de audio o imagen"""
    try:
        # Descargar archivo
        response = requests.get(url_archivo)
        if response.status_code != 200:
            return "Error al descargar el archivo."
        
        # Guardar temporalmente
        extension = 'jpg' if tipo_archivo == 'image' else 'wav'
        archivo_temp = f"temp_{usuario_id}.{extension}"
        
        with open(archivo_temp, 'wb') as f:
            f.write(response.content)
        
        # Subir a Gemini
        file_ref = genai.upload_file(archivo_temp)
        
        # Crear prompt contextualizado con system prompt
        prompt = f"""
        {SYSTEM_PROMPT}
        
        DATOS DEL INVENTARIO:
        {df.to_string() if not df.empty else "No hay datos de inventario"}
        
        TAREA: Analiza este {tipo_archivo} y busca informaci√≥n relacionada en el inventario.
        
        Responde siguiendo las reglas establecidas y usando solo la informaci√≥n del inventario.
        """
        
        # Obtener sesi√≥n de chat
        chat = obtener_sesion_chat(usuario_id)
        
        # Enviar mensaje con archivo
        response = chat.send_message([prompt, file_ref])
        
        # Guardar sesi√≥n
        guardar_sesion_chat(usuario_id, chat)
        
        # Limpiar archivo temporal
        os.remove(archivo_temp)
        
        return response.text
        
    except Exception as e:
        logger.error(f"Error procesando archivo multimodal: {e}")
        return "Error al procesar el archivo. Intenta de nuevo."

@app.route("/whatsapp", methods=['POST'])
def whatsapp_webhook():
    """Webhook principal para recibir mensajes de WhatsApp"""
    try:
        # Obtener datos del mensaje
        incoming_msg = request.values.get('Body', '')
        from_number = request.values.get('From', '')
        media_url = request.values.get('MediaUrl0', '')
        media_content_type = request.values.get('MediaContentType0', '')
        
        logger.info(f"Mensaje recibido de {from_number}: {incoming_msg}")
        
        # Cargar inventario
        df = cargar_inventario()
        
        # Determinar tipo de mensaje
        if media_url:
            # Mensaje con archivo (imagen o audio)
            if 'image' in media_content_type:
                respuesta = procesar_archivo_multimodal(media_url, 'image', from_number, df)
            elif 'audio' in media_content_type:
                respuesta = procesar_archivo_multimodal(media_url, 'audio', from_number, df)
            else:
                respuesta = MENSAJES["archivo_no_soportado"]
        else:
            # Mensaje de texto
            logger.info(f"Procesando mensaje de texto: '{incoming_msg}'")
            # Detectar saludos de manera m√°s flexible
            saludo_detectado = any(saludo in incoming_msg.lower() for saludo in ['hola', 'hi', 'hello', 'buenos d√≠as', 'buenas'])
            if saludo_detectado:
                logger.info("Mensaje reconocido como saludo")
                respuesta = CONFIG["saludo_personalizado"]
            else:
                # Verificar si hay modelo disponible antes de consultar
                modelo_funcional = obtener_modelo_funcional()
                if not modelo_funcional:
                    logger.warning("No hay modelo disponible, usando respuesta de fallback")
                    # Respuesta b√°sica sin IA para consultas de inventario
                    if any(palabra in incoming_msg.lower() for palabra in ['inventario', 'producto', 'stock', 'precio']):
                        respuesta = f"""üì¶ **Inventario Disponible** (10 productos)

Los productos disponibles incluyen:
‚Ä¢ Auriculares Sony WH-1000XM4
‚Ä¢ iPhone 15 Pro
‚Ä¢ Samsung Galaxy S24
‚Ä¢ MacBook Pro M3
‚Ä¢ iPad Air
‚Ä¢ Y m√°s productos...

‚ùå **Servicio de IA temporalmente no disponible**
Para consultas espec√≠ficas sobre precios y stock, intenta m√°s tarde."""
                    else:
                        respuesta = "‚ùå Lo siento, el servicio de IA no est√° disponible en este momento. Puedes intentar m√°s tarde o contactar al administrador."
                else:
                    # Consultar inventario con Gemini
                    logger.info("Consultando inventario con Gemini")
                    respuesta = consultar_excel(incoming_msg, df)
                    logger.info(f"Respuesta generada: {respuesta[:100]}...")
        
        # Crear respuesta TwiML
        logger.info(f"Enviando respuesta: {respuesta[:100]}...")
        resp = MessagingResponse()
        resp.message(respuesta)
        
        return str(resp)
        
    except Exception as e:
        logger.error(f"Error en webhook: {e}")
        resp = MessagingResponse()
        resp.message("Lo siento, ocurri√≥ un error. Intenta de nuevo.")
        return str(resp)

@app.route("/health", methods=['GET'])
def health_check():
    """Endpoint de salud para verificar que el servidor funciona"""
    modelo_funcional = obtener_modelo_funcional()
    return {
        "status": "ok" if modelo_funcional else "error",
        "timestamp": datetime.now().isoformat(),
        "gemini_model": modelo_funcional or "none",
        "model_available": bool(modelo_funcional),
        "available_models": listar_modelos_disponibles()
    }

@app.route("/debug", methods=['GET'])
def debug_api():
    """Endpoint de debug para probar la API de Gemini"""
    try:
        # Probar la API key
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return {"error": "GEMINI_API_KEY no configurada"}
        
        # Listar modelos
        models = genai.list_models()
        available_models = []
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append({
                    "name": model.name,
                    "display_name": model.display_name,
                    "supported_methods": list(model.supported_generation_methods)
                })
        
        # Probar un modelo simple
        test_result = None
        modelos_debug = ['gemini-1.5-flash-latest', 'gemini-1.5-flash', 'gemini-2.0-flash']
        
        for modelo_debug in modelos_debug:
            try:
                model = genai.GenerativeModel(modelo_debug)
                response = model.generate_content("Hola, ¬øfuncionas?")
                test_result = {
                    "success": True,
                    "response": response.text[:100],
                    "model_used": modelo_debug
                }
                break  # Si funciona, salir del bucle
            except Exception as e:
                test_result = {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "model_tested": modelo_debug
                }
                continue
        
        return {
            "api_key_configured": bool(api_key),
            "api_key_length": len(api_key) if api_key else 0,
            "available_models": available_models,
            "test_result": test_result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {"error": str(e), "timestamp": datetime.now().isoformat()}

@app.route("/", methods=['GET'])
def home():
    """P√°gina de inicio"""
    return render_template('index.html')

@app.route("/config", methods=['GET'])
def config_page():
    """P√°gina de configuraci√≥n del agente"""
    return render_template('config.html')

@app.route("/api/config", methods=['GET'])
def get_config():
    """Obtener configuraci√≥n actual del agente"""
    try:
        config = {
            "system_prompt": SYSTEM_PROMPT,
            "config_agente": CONFIG,
            "limites": LIMITES,
            "mensajes": MENSAJES
        }
        return jsonify({"success": True, "config": config})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)})

@app.route("/api/config", methods=['POST'])
def save_config():
    """Guardar nueva configuraci√≥n del agente"""
    global SYSTEM_PROMPT, CONFIG, LIMITES, MENSAJES
    
    try:
        data = request.get_json()
        
        # Validar datos
        if not data:
            return jsonify({"success": False, "error": "No se recibieron datos"})
        
        # Guardar en archivo de configuraci√≥n
        config_data = {
            "system_prompt": data.get("system_prompt", SYSTEM_PROMPT),
            "config_agente": data.get("config_agente", CONFIG),
            "limites": data.get("limites", LIMITES),
            "mensajes": data.get("mensajes", MENSAJES)
        }
        
        # Guardar en archivo JSON
        with open('config_dinamico.json', 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        # Recargar configuraci√≥n en memoria
        SYSTEM_PROMPT = config_data["system_prompt"]
        CONFIG = config_data["config_agente"]
        LIMITES = config_data["limites"]
        MENSAJES = config_data["mensajes"]
        
        return jsonify({"success": True, "message": "Configuraci√≥n guardada exitosamente"})
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)})

@app.route("/api/config/reset", methods=['POST'])
def reset_config():
    """Restablecer configuraci√≥n a valores por defecto"""
    global SYSTEM_PROMPT, CONFIG, LIMITES, MENSAJES
    
    try:
        # Recargar configuraci√≥n desde archivos originales
        SYSTEM_PROMPT = obtener_system_prompt()
        CONFIG = obtener_configuracion()
        LIMITES = obtener_limites()
        MENSAJES = obtener_mensajes()
        
        # Eliminar archivo de configuraci√≥n din√°mica si existe
        if os.path.exists('config_dinamico.json'):
            os.remove('config_dinamico.json')
        
        return jsonify({"success": True, "message": "Configuraci√≥n restablecida a valores por defecto"})
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)})

@app.route("/api/proveedor", methods=['GET'])
def get_proveedor():
    """Obtener el proveedor de IA actual"""
    return jsonify({
        "success": True, 
        "proveedor": PROVEEDOR_IA_ACTIVO,
        "opciones": ["gemini", "siliconflow"]
    })

@app.route("/api/proveedor", methods=['POST'])
def set_proveedor():
    """Cambiar el proveedor de IA"""
    global PROVEEDOR_IA_ACTIVO
    
    try:
        data = request.get_json()
        nuevo_proveedor = data.get('proveedor')
        
        if nuevo_proveedor not in ['gemini', 'siliconflow']:
            return jsonify({"success": False, "error": "Proveedor no v√°lido"})
        
        PROVEEDOR_IA_ACTIVO = nuevo_proveedor
        logger.info(f"Proveedor de IA cambiado a: {nuevo_proveedor}")
        
        return jsonify({
            "success": True, 
            "message": f"Proveedor cambiado a {nuevo_proveedor}",
            "proveedor": PROVEEDOR_IA_ACTIVO
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)})

if __name__ == '__main__':
    # Verificar configuraci√≥n
    if not os.getenv('GEMINI_API_KEY'):
        logger.error("GEMINI_API_KEY no configurada")
        exit(1)
    
    if not os.getenv('TWILIO_ACCOUNT_SID'):
        logger.error("TWILIO_ACCOUNT_SID no configurada")
        exit(1)
    
    # Verificar que el modelo est√© disponible (no cr√≠tico para el inicio)
    modelo_disponible, modelo_usado = verificar_modelo_disponible()
    if not modelo_disponible:
        logger.warning("El modelo de Gemini no est√° disponible al inicio, pero la aplicaci√≥n continuar√°")
        logger.warning("Se intentar√° encontrar un modelo funcional cuando se reciba el primer mensaje")
    else:
        logger.info(f"Modelo verificado al inicio: {modelo_usado}")
    
    # Obtener puerto de Railway o usar 5000 por defecto
    port = int(os.getenv('PORT', 5000))
    
    logger.info("Iniciando servidor Flask...")
    app.run(debug=False, host='0.0.0.0', port=port)



